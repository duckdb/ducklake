# name: test/sql/compaction/merge_adjacent_return_value.test
# description: test that compaction functions return the number of files processed
# group: [compaction]

require ducklake

require parquet

test-env DUCKLAKE_CONNECTION __TEST_DIR__/{UUID}.db

test-env DATA_PATH __TEST_DIR__

statement ok
ATTACH 'ducklake:${DUCKLAKE_CONNECTION}' AS ducklake (DATA_PATH '${DATA_PATH}/merge_adjacent_return_value/')

statement ok
USE ducklake;

statement ok
CREATE TABLE test_table (id INTEGER, data VARCHAR);

# Insert data in 4 separate transactions to create 4 files
statement ok
INSERT INTO test_table SELECT i, 'data' FROM range(10) t(i);

statement ok
INSERT INTO test_table SELECT i, 'data' FROM range(10, 20) t(i);

statement ok
INSERT INTO test_table SELECT i, 'data' FROM range(20, 30) t(i);

statement ok
INSERT INTO test_table SELECT i, 'data' FROM range(30, 40) t(i);

# Verify we have 4 files
query I
SELECT count(*) FROM __ducklake_metadata_ducklake.ducklake_data_file WHERE end_snapshot IS NULL;
----
4

# Set a large target file size so all files merge into one
statement ok
CALL ducklake.set_option('target_file_size', '10MB');

# Run merge and check return value - should return 4 (all files merged into one)
query I
SELECT files_processed FROM ducklake_merge_adjacent_files('ducklake', 'test_table');
----
4

# Verify we now have 1 file
query I
SELECT count(*) FROM __ducklake_metadata_ducklake.ducklake_data_file WHERE end_snapshot IS NULL;
----
1

# Verify data is still correct
query I
SELECT count(*) FROM test_table;
----
40

# Test multiple return rows (multiple compaction operations)
statement ok
CREATE TABLE multi_merge (id INTEGER, data VARCHAR);

# Insert data in 6 separate transactions to create 6 files
# Use larger data to create bigger files
statement ok
INSERT INTO multi_merge SELECT i, repeat('x', 100) FROM range(100) t(i);

statement ok
INSERT INTO multi_merge SELECT i, repeat('x', 100) FROM range(100, 200) t(i);

statement ok
INSERT INTO multi_merge SELECT i, repeat('x', 100) FROM range(200, 300) t(i);

statement ok
INSERT INTO multi_merge SELECT i, repeat('x', 100) FROM range(300, 400) t(i);

statement ok
INSERT INTO multi_merge SELECT i, repeat('x', 100) FROM range(400, 500) t(i);

statement ok
INSERT INTO multi_merge SELECT i, repeat('x', 100) FROM range(500, 600) t(i);

# Verify we have 6 files
query I
SELECT count(*) FROM __ducklake_metadata_ducklake.ducklake_data_file df
JOIN __ducklake_metadata_ducklake.ducklake_table t ON df.table_id = t.table_id
WHERE df.end_snapshot IS NULL AND t.table_name = 'multi_merge';
----
6

# Set target file size to 3KB - each file is ~1.2KB, so we should get 2 output files
# (3 files merged into each)
statement ok
CALL ducklake.set_option('target_file_size', '3KB');

# Run merge - should return 2 rows (one per output file), each showing 3 files merged
# Use SUM to verify total files merged = 6
query I
SELECT SUM(files_processed) FROM ducklake_merge_adjacent_files('ducklake', 'multi_merge');
----
6

# Verify we now have 2 files
query I
SELECT count(*) FROM __ducklake_metadata_ducklake.ducklake_data_file df
JOIN __ducklake_metadata_ducklake.ducklake_table t ON df.table_id = t.table_id
WHERE df.end_snapshot IS NULL AND t.table_name = 'multi_merge';
----
2

# Verify data is still correct
query I
SELECT count(*) FROM multi_merge;
----
600

# Test ducklake_rewrite_data_files return value
statement ok
CREATE TABLE rewrite_test (id INTEGER, data VARCHAR);

# Insert data in 3 separate transactions to create 3 files
statement ok
INSERT INTO rewrite_test SELECT i, 'data' FROM range(100) t(i);

statement ok
INSERT INTO rewrite_test SELECT i, 'data' FROM range(100, 200) t(i);

statement ok
INSERT INTO rewrite_test SELECT i, 'data' FROM range(200, 300) t(i);

# Delete some rows from each file to create delete files
statement ok
DELETE FROM rewrite_test WHERE id % 10 = 0;

# Verify we have 3 data files with deletes
query I
SELECT count(*) FROM __ducklake_metadata_ducklake.ducklake_data_file df
JOIN __ducklake_metadata_ducklake.ducklake_table t ON df.table_id = t.table_id
WHERE df.end_snapshot IS NULL AND t.table_name = 'rewrite_test';
----
3

# Set large target file size so all files are candidates
statement ok
CALL ducklake.set_option('target_file_size', '10MB');

# Run rewrite with delete_threshold => 0 (rewrite any file with deletes)
# Default threshold is 0.95 which requires 95% of rows to be deleted
query I
SELECT files_processed FROM ducklake_rewrite_data_files('ducklake', 'rewrite_test', delete_threshold => 0);
----
3

# Verify we now have 1 file (all rewritten into one)
query I
SELECT count(*) FROM __ducklake_metadata_ducklake.ducklake_data_file df
JOIN __ducklake_metadata_ducklake.ducklake_table t ON df.table_id = t.table_id
WHERE df.end_snapshot IS NULL AND t.table_name = 'rewrite_test';
----
1

# Verify data is still correct (300 - 30 deleted = 270)
query I
SELECT count(*) FROM rewrite_test;
----
270
